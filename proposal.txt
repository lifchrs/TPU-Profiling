Group 4 Project 7 Project Proposal
Kevin Chen, Kalyan, Aryan Joshi, Adhitya Polavaram
Project selection
Our project is dedicated to an in-depth empirical investigation into the performance characteristics of large language models (LLMs) during the inference phase, specifically focusing on how different parallelization strategies impact communication patterns, scheduling overheads, and overall computational efficiency. This will be done using TorchXLA on Google TPUs.
Scope
Modern LLM inference workloads rely heavily on distributed computation across multiple accelerators. Each inference step involves collective communication operations (e.g., AllReduce, AllGather, ReduceScatter) to exchange activations, logits, or key value caches across devices. These operations can become bottlenecks, dominating runtime and impacting scalability, especially as model and batch sizes increase.
While GPU-based systems have been extensively studied using libraries like NCCL, TPUs leverage a different interconnect and software stack based on XLA (Accelerated Linear Algebra). TorchXLA provides PyTorch users an interface to this backend, but the communication behavior, scheduling, and efficiency under different parallelism strategies (data, tensor, pipeline, and possibly expert parallelism) remain underexplored. This project aims to profile and analyze the communication characteristics of LLM inference on TPUs using TorchXLA, focusing on identifying bandwidth utilization, kernel scheduling behavior, and efficiency differences across parallelism modes. The work will contribute to CCL-Bench by providing TPU-based inference traces. 
The core areas of our analysis include:
Communication Analysis: Quantifying and characterizing the inter-device communication volume, frequency, and latency for the different parallelism paradigms. This will involve instrumenting the TorchXLA runtime to track and profile the underlying collective operations (e.g., all-reduce, all-gather, scatter/gather) essential for synchronizing states or exchanging tensor fragments across the TPU cores.
Scheduling and Synchronization Overheads: Measuring the impact of parallelization on the runtime scheduling of computational kernels and the necessary synchronization points. We will assess how the different schemes affect load balancing across the TPU cores and the degree to which idle time is introduced due to waiting for data or synchronization barriers.
Efficiency and Throughput: Determining the latency and throughput metrics such as Time-to-First-Token (TTFT), Time-per-Output-Token (TPOT), and end-to-end latency for a fixed LLM size and batch size under each parallelism configuration. The ultimate goal is to identify the most efficient parallelization strategy for serving LLMs on Google TPU infrastructure, providing actionable insights for optimizing large-scale AI deployment.

Implementation Plans
Our team plans to use either Google Colab or Google Cloud to access TPUs for our experiments. Google Colab offers a simple, collaborative, and quick-to-start environment. Its shared notebook interface makes it easy for team members to work asynchronously, with documentation, code, and outputs visible in one place. This structure encourages transparency and shared understanding as we iterate on experiments together. However, Colab’s main limitation is that it is stateless. This makes maintaining state between sessions difficult, requiring us to repeatedly reinstall packages and reload models.
In contrast, Google Cloud provides a more reliable and persistent environment. By renting a virtual machine (VM) equipped with TPUs, we can maintain a consistent setup where libraries, cached data, and models remain available between sessions. Team members can SSH into the same shared instance, ensuring stable collaboration without losing work or configuration. The main drawback, however, is cost as running a TPU-enabled VM continuously can become expensive, especially during extended experimentation periods or when scaling up to larger model sizes.
After deciding on which Google service to use, we will begin by developing an inference harness capable of various LLMs (see Expected Results section for exact models) under configurable parallelism modes on TPUs. We will evaluate different parallelization strategies, including tensor parallelism and sequence parallelism, to understand their performance trade-offs on TPUs. TorchXLA’s Single Program Multiple Data (SPMD) model provides built-in support for tensor-style sharding through the Mesh and PartitionSpec APIs, allowing tensors to be distributed across TPU cores. However, explicit sequence parallelism—where the sequence dimension is partitioned across devices—is not fully implemented in TorchXLA and will require custom handling of data layout and collective operations.
Once the inference infrastructure is established, we will conduct a series of systematic experiments that vary key parameters such as model, batch size, sequence length, parallelization strategy, and the number of TPU cores. For each configuration, we will execute controlled inference runs using our TorchXLA-based harness, which will generate structured traces capturing detailed runtime information (see section Expected Results). These traces will then be processed at CCL-Bench, which will analyze them to produce a comprehensive suite of statistics summarizing scaling efficiency, communication overhead, and synchronization behavior across parallelism modes. These results will form a dataset and interpretive report providing insights into how model size, workload parameters, and parallelization strategies impact LLM inference performance on TPUs via TorchXLA.
Expected Results
The 3 benchmark models we will utilize to run inference include DeepSeek-V3.1-Base, Qwen-32b, Deepseek 4.1, DeepSeek-V2-Lite. The reason we chose this variety of models is to assess communication performances across large and smaller models. For each model, we hope to present. We expect larger models to exhibit higher communication overhead and reduced overlap between computation and communication, while smaller models should show the contrary. Overall, we are aiming to gather traces that highlight differences in collective operation frequency, bandwidth utilization and communication to computation ratios across our different models. 
For each model and parallelism configuration, we will collect detailed execution traces using the TPU Profiler available through TorchXLA, which provides functionality similar to NVIDIA’s Nsight Systems but tailored for TPU workloads. Profiling will be enabled for every inference run to capture fine-grained, kernel-level timelines of both computation and communication activities. These traces will include the start and end times of key operations, such as matrix multiplications, attention kernels, and collective communication operations (AllReduce, AllGather, ReduceScatter, AllToAll), recorded across all TPU cores. Each run will also include markers to identify major phases of execution, including prefill and decode, as well as individual model layers. The collected traces will be saved as Chrome trace files and organized by model and parallelism setting for subsequent analysis. Using these profiles, we will extract and summarize core performance metrics - such as TTFT, TPOT, achieved bandwidth, communication-to-compute ratio, traffic intervals within and across parallelisms, and observed idle or bubble time. This profiling data will form the foundation for evaluating how tensor and pipeline parallelism impact communication patterns, scheduling efficiency, and overall inference performance on TPUs.

Expected Hours
Although our project focuses on inference rather than training, we estimate spending approximately 15-20 TPU hours per person. Most of this time will be spent on model computation, environmental setup, configuration, and debugging within TorchXLA on Google TPU. We also expect additional time for running multiple inference configurations to collect valid profiling traces or rerunning if we have any profiling errors. 
